{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 22, 15, 10]\n",
      "Action  0 3  ->  (0, 0.0, False, {'prob': 1.0})\n",
      "Action  1 1  ->  (1, 0.0, False, {'prob': 1.0})\n",
      "Action  2 1  ->  (2, 0.0, False, {'prob': 1.0})\n",
      "Action  1 3  ->  (1, 0.0, False, {'prob': 1.0})\n",
      "Action  2 1  ->  (2, 0.0, False, {'prob': 1.0})\n",
      "Action  1 3  ->  (1, 0.0, False, {'prob': 1.0})\n",
      "Action  1 0  ->  (1, 0.0, False, {'prob': 1.0})\n",
      "Action  7 2  ->  (7, 0.0, False, {'prob': 1.0})\n",
      "Action  13 2  ->  (13, 0.0, False, {'prob': 1.0})\n",
      "Action  14 1  ->  (14, 0.0, False, {'prob': 1.0})\n",
      "Action  8 0  ->  (8, 0.0, False, {'prob': 1.0})\n",
      "Action  9 1  ->  (9, 0.0, False, {'prob': 1.0})\n",
      "Action  3 0  ->  (3, 0.0, False, {'prob': 1.0})\n",
      "Action  4 1  ->  (4, 0.0, False, {'prob': 1.0})\n",
      "Action  4 0  ->  (4, 0.0, False, {'prob': 1.0})\n",
      "Action  4 0  ->  (4, 0.0, False, {'prob': 1.0})\n",
      "Action  4 0  ->  (4, 0.0, False, {'prob': 1.0})\n",
      "Action  4 0  ->  (4, 0.0, False, {'prob': 1.0})\n",
      "Action  5 1  ->  (5, 0.0, False, {'prob': 1.0})\n",
      "Action  11 2  ->  (11, 0.0, False, {'prob': 1.0})\n",
      "Action  5 0  ->  (5, 0.0, False, {'prob': 1.0})\n",
      "Action  5 0  ->  (5, 0.0, False, {'prob': 1.0})\n",
      "Action  5 0  ->  (5, 0.0, False, {'prob': 1.0})\n",
      "Action  5 1  ->  (5, 0.0, False, {'prob': 1.0})\n",
      "Action  11 2  ->  (11, 0.0, False, {'prob': 1.0})\n",
      "Action  10 3  ->  (10, -1.0, False, {'prob': 1.0})\n",
      "Action  10 0  ->  (10, 0.0, True, {'prob': 1.0})\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gym.envs.toy_text import discrete\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from gym.envs.classic_control import rendering\n",
    "\n",
    "CELL_SIZE = 100\n",
    "MARGIN = 10\n",
    "\n",
    "\n",
    "def get_coords(row, col, loc='center'):\n",
    "    xc = (col + 1.5) * CELL_SIZE\n",
    "    yc = (row + 1.5) * CELL_SIZE\n",
    "    if loc == 'center':\n",
    "        return xc, yc\n",
    "    elif loc == 'interior_corners':\n",
    "        half_size = CELL_SIZE//2 - MARGIN\n",
    "        xl, xr = xc - half_size, xc + half_size\n",
    "        yt, yb = xc - half_size, xc + half_size\n",
    "        return [(xl, yt), (xr, yt), (xr, yb), (xl, yb)]\n",
    "    elif loc == 'interior_triangle':\n",
    "        x1, y1 = xc, yc + CELL_SIZE//3\n",
    "        x2, y2 = xc + CELL_SIZE//3, yc - CELL_SIZE//3\n",
    "        x3, y3 = xc - CELL_SIZE//3, yc - CELL_SIZE//3\n",
    "        return [(x1, y1), (x2, y2), (x3, y3)]\n",
    "\n",
    "\n",
    "def draw_object(coords_list):\n",
    "    if len(coords_list) == 1:  # -> circle\n",
    "        obj = rendering.make_circle(int(0.45*CELL_SIZE))\n",
    "        obj_transform = rendering.Transform()\n",
    "        obj.add_attr(obj_transform)\n",
    "        obj_transform.set_translation(*coords_list[0])\n",
    "        obj.set_color(0.2, 0.2, 0.2)  # -> black\n",
    "    elif len(coords_list) == 3:  # -> triangle\n",
    "        obj = rendering.FilledPolygon(coords_list)\n",
    "        obj.set_color(0.9, 0.6, 0.2)  # -> yellow\n",
    "    elif len(coords_list) > 3:  # -> polygon\n",
    "        obj = rendering.FilledPolygon(coords_list)\n",
    "        obj.set_color(0.4, 0.4, 0.8)  # -> blue\n",
    "    return obj\n",
    "\n",
    "\n",
    "class GridWorldEnv(discrete.DiscreteEnv):\n",
    "    def __init__(self, num_rows=4, num_cols=6, delay=0.05):\n",
    "        self.num_rows = num_rows\n",
    "        self.num_cols = num_cols\n",
    "\n",
    "        self.delay = delay\n",
    "\n",
    "        move_up = lambda row, col: (max(row - 1, 0), col)\n",
    "        move_down = lambda row, col: (min(row + 1, num_rows - 1), col)\n",
    "        move_left = lambda row, col: (row, max(col - 1, 0))\n",
    "        move_right = lambda row, col: (row, min(col + 1, num_cols - 1))\n",
    "\n",
    "        self.action_defs = {0: move_up, 1: move_right,\n",
    "                            2: move_down, 3: move_left}\n",
    "\n",
    "        nS = num_cols * num_rows\n",
    "        nA = len(self.action_defs)\n",
    "        self.grid2state_dict = {(s // num_cols, s % num_cols): s\n",
    "                                for s in range(nS)}\n",
    "        self.state2grid_dict = {s: (s // num_cols, s % num_cols)\n",
    "                                for s in range(nS)}\n",
    "\n",
    "        gold_cell = (num_rows // 2, num_cols - 2)\n",
    "\n",
    "        trap_cells = [((gold_cell[0] + 1), gold_cell[1]),\n",
    "                      (gold_cell[0], gold_cell[1] - 1),\n",
    "                      ((gold_cell[0] - 1), gold_cell[1])]\n",
    "\n",
    "        gold_state = self.grid2state_dict[gold_cell]\n",
    "        trap_states = [self.grid2state_dict[(r, c)]\n",
    "                       for (r, c) in trap_cells]\n",
    "        self.terminal_states = [gold_state] + trap_states\n",
    "        print(self.terminal_states)\n",
    "\n",
    "        P = defaultdict(dict)\n",
    "        for s in range(nS):\n",
    "            row, col = self.state2grid_dict[s]\n",
    "            P[s] = defaultdict(list)\n",
    "            for a in range(nA):\n",
    "                action = self.action_defs[a]\n",
    "                next_s = self.grid2state_dict[action(row, col)]\n",
    "\n",
    "                # Terminal state\n",
    "                if self.is_terminal(next_s):\n",
    "                    r = (1.0 if next_s == self.terminal_states[0]\n",
    "                         else -1.0)\n",
    "                else:\n",
    "                    r = 0.0\n",
    "                if self.is_terminal(s):\n",
    "                    done = True\n",
    "                    next_s = s\n",
    "                else:\n",
    "                    done = False\n",
    "                P[s][a] = [(1.0, next_s, r, done)]\n",
    "\n",
    "        isd = np.zeros(nS)\n",
    "        isd[0] = 1.0\n",
    "\n",
    "        super(GridWorldEnv, self).__init__(nS, nA, P, isd)\n",
    "\n",
    "        self.viewer = None\n",
    "        self._build_display(gold_cell, trap_cells)\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        return state in self.terminal_states\n",
    "\n",
    "    def _build_display(self, gold_cell, trap_cells):\n",
    "\n",
    "        screen_width = (self.num_cols + 2) * CELL_SIZE\n",
    "        screen_height = (self.num_rows + 2) * CELL_SIZE\n",
    "        self.viewer = rendering.Viewer(screen_width,\n",
    "                                       screen_height)\n",
    "\n",
    "        all_objects = []\n",
    "\n",
    "        bp_list = [\n",
    "            (CELL_SIZE - MARGIN, CELL_SIZE - MARGIN),\n",
    "            (screen_width - CELL_SIZE + MARGIN, CELL_SIZE - MARGIN),\n",
    "            (screen_width - CELL_SIZE + MARGIN,\n",
    "             screen_height - CELL_SIZE + MARGIN),\n",
    "            (CELL_SIZE - MARGIN, screen_height - CELL_SIZE + MARGIN)\n",
    "        ]\n",
    "        border = rendering.PolyLine(bp_list, True)\n",
    "        border.set_linewidth(5)\n",
    "        all_objects.append(border)\n",
    "\n",
    "        for col in range(self.num_cols + 1):\n",
    "            x1, y1 = (col + 1) * CELL_SIZE, CELL_SIZE\n",
    "            x2, y2 = (col + 1) * CELL_SIZE, \\\n",
    "                     (self.num_rows + 1) * CELL_SIZE\n",
    "            line = rendering.PolyLine([(x1, y1), (x2, y2)], False)\n",
    "            all_objects.append(line)\n",
    "\n",
    "        for row in range(self.num_rows + 1):\n",
    "            x1, y1 = CELL_SIZE, (row + 1) * CELL_SIZE\n",
    "            x2, y2 = (self.num_cols + 1) * CELL_SIZE, \\\n",
    "                     (row + 1) * CELL_SIZE\n",
    "            line = rendering.PolyLine([(x1, y1), (x2, y2)], False)\n",
    "            all_objects.append(line)\n",
    "\n",
    "        for cell in trap_cells:\n",
    "            trap_coords = get_coords(*cell, loc='center')\n",
    "            all_objects.append(draw_object([trap_coords]))\n",
    "\n",
    "        gold_coords = get_coords(*gold_cell,\n",
    "                                 loc='interior_triangle')\n",
    "        all_objects.append(draw_object(gold_coords))\n",
    "\n",
    "        if (os.path.exists('robot-coordinates.pkl') and CELL_SIZE == 100):\n",
    "            agent_coords = pickle.load(\n",
    "                open('robot-coordinates.pkl', 'rb'))\n",
    "            starting_coords = get_coords(0, 0, loc='center')\n",
    "            agent_coords += np.array(starting_coords)\n",
    "        else:\n",
    "            agent_coords = get_coords(0, 0, loc='interior_corners')\n",
    "        agent = draw_object(agent_coords)\n",
    "        self.agent_trans = rendering.Transform()\n",
    "        agent.add_attr(self.agent_trans)\n",
    "        all_objects.append(agent)\n",
    "\n",
    "        for obj in all_objects:\n",
    "            self.viewer.add_geom(obj)\n",
    "\n",
    "    def render(self, mode='human', done=False):\n",
    "        if done:\n",
    "            sleep_time = 1\n",
    "        else:\n",
    "            sleep_time = self.delay\n",
    "        x_coord = self.s % self.num_cols\n",
    "        y_coord = self.s // self.num_cols\n",
    "        x_coord = (x_coord + 0) * CELL_SIZE\n",
    "        y_coord = (y_coord + 0) * CELL_SIZE\n",
    "        self.agent_trans.set_translation(x_coord, y_coord)\n",
    "        rend = self.viewer.render(\n",
    "            return_rgb_array=(mode == 'rgb_array'))\n",
    "        time.sleep(sleep_time)\n",
    "        return rend\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env = GridWorldEnv(5, 6)\n",
    "    for i in range(1):\n",
    "        s = env.reset()\n",
    "        env.render(mode='human', done=False)\n",
    "\n",
    "        while True:\n",
    "            action = np.random.choice(env.nA)\n",
    "            res = env.step(action)\n",
    "            print('Action ', env.s, action, ' -> ', res)\n",
    "            env.render(mode='human', done=res[2])\n",
    "            if res[2]:\n",
    "                break\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(\n",
    "            self, env,\n",
    "            learning_rate=0.01,\n",
    "            discount_factor=0.9,\n",
    "            epsilon_greedy=0.9,\n",
    "            epsilon_min=0.1,\n",
    "            epsilon_decay=0.95):\n",
    "        self.env = env\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        self.epsilon = epsilon_greedy\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "        # Define the q_table\n",
    "        self.q_table = defaultdict(lambda: np.zeros(self.env.nA))\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            action = np.random.choice(self.env.nA)\n",
    "        else:\n",
    "            q_vals = self.q_table[state]\n",
    "            perm_actions = np.random.permutation(self.env.nA)\n",
    "            q_vals = [q_vals[a] for a in perm_actions]\n",
    "            perm_q_argmax = np.argmax(q_vals)\n",
    "            action = perm_actions[perm_q_argmax]\n",
    "        return action\n",
    "\n",
    "    def _learn(self, transition):\n",
    "        s, a, r, next_s, done = transition\n",
    "        q_val = self.q_table[s][a]\n",
    "        if done:\n",
    "            q_target = r\n",
    "        else:\n",
    "            q_target = r + self.gamma*np.max(self.q_table[next_s])\n",
    "\n",
    "        # Update the q_table\n",
    "        self.q_table[s][a] += self.lr * (q_target - q_val)\n",
    "\n",
    "        # Adjust the epislon\n",
    "        self._adjust_epsilon()\n",
    "\n",
    "    def _adjust_epsilon(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gridworld_env'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-97158f524264>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgridworld_env\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGridWorldEnv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0magent\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAgent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnamedtuple\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gridworld_env'"
     ]
    }
   ],
   "source": [
    "from gridworld_env import GridWorldEnv\n",
    "from agent import Agent\n",
    "from collections import namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "Transition = namedtuple(\n",
    "    'Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "\n",
    "def run_qlearning(agent, env, num_episodes=50):\n",
    "    history = []\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        env.render(mode='human')\n",
    "        final_reward, n_moves = 0.0, 0\n",
    "        while True:\n",
    "            action = agent.choose_action(state)\n",
    "            next_s, reward, done, _ = env.step(action)\n",
    "            agent._learn(Transition(state, action, reward,\n",
    "                                    next_s, done))\n",
    "            env.render(mode='human', done=done)\n",
    "            state = next_s\n",
    "            n_moves += 1\n",
    "            if done:\n",
    "                break\n",
    "            final_reward = reward\n",
    "        history.append((n_moves, final_reward))\n",
    "        print('Episode %d: Reward %.1f #Moves %d'\n",
    "              % (episode, final_reward, n_moves))\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "def plot_learning_history(history):\n",
    "    fig = plt.figure(1, figsize=(14, 10))\n",
    "    ax = fig.add_subplot(2, 1, 1)\n",
    "    episodes = np.arange(len(history))\n",
    "    moves = np.array([h[0] for h in history])\n",
    "    plt.plot(episodes, moves, lw=4,\n",
    "             marker=\"o\", markersize=10)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "    plt.xlabel('Episodes', size=20)\n",
    "    plt.ylabel('# moves', size=20)\n",
    "\n",
    "    ax = fig.add_subplot(2, 1, 2)\n",
    "    rewards = np.array([h[1] for h in history])\n",
    "    plt.step(episodes, rewards, lw=4)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "    plt.xlabel('Episodes', size=20)\n",
    "    plt.ylabel('Final rewards', size=20)\n",
    "    plt.savefig('q-learning-history.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env = GridWorldEnv(num_rows=5, num_cols=6)\n",
    "    agent = Agent(env)\n",
    "    history = run_qlearning(agent, env)\n",
    "    env.close()\n",
    "\n",
    "    plot_learning_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0/200, Total reward: 27\n",
      "Episode: 1/200, Total reward: 9\n",
      "Episode: 2/200, Total reward: 7\n",
      "Episode: 3/200, Total reward: 9\n",
      "Episode: 4/200, Total reward: 9\n",
      "Episode: 5/200, Total reward: 8\n",
      "Episode: 6/200, Total reward: 7\n",
      "Episode: 7/200, Total reward: 8\n",
      "Episode: 8/200, Total reward: 8\n",
      "Episode: 9/200, Total reward: 14\n",
      "Episode: 10/200, Total reward: 8\n",
      "Episode: 11/200, Total reward: 7\n",
      "Episode: 12/200, Total reward: 8\n",
      "Episode: 13/200, Total reward: 8\n",
      "Episode: 14/200, Total reward: 7\n",
      "Episode: 15/200, Total reward: 7\n",
      "Episode: 16/200, Total reward: 9\n",
      "Episode: 17/200, Total reward: 11\n",
      "Episode: 18/200, Total reward: 9\n",
      "Episode: 19/200, Total reward: 9\n",
      "Episode: 20/200, Total reward: 8\n",
      "Episode: 21/200, Total reward: 8\n",
      "Episode: 22/200, Total reward: 9\n",
      "Episode: 23/200, Total reward: 8\n",
      "Episode: 24/200, Total reward: 15\n",
      "Episode: 25/200, Total reward: 24\n",
      "Episode: 26/200, Total reward: 24\n",
      "Episode: 27/200, Total reward: 14\n",
      "Episode: 28/200, Total reward: 15\n",
      "Episode: 29/200, Total reward: 17\n",
      "Episode: 30/200, Total reward: 28\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-23191cb9f3be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    152\u001b[0m                       % (e, EPISODES, i))\n\u001b[0;32m    153\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    155\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[0mplot_learning_history\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_rewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-23191cb9f3be>\u001b[0m in \u001b[0;36mreplay\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[0msamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_learn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-23191cb9f3be>\u001b[0m in \u001b[0;36m_learn\u001b[1;34m(self, batch_samples)\u001b[0m\n\u001b[0;32m     75\u001b[0m                             )\n\u001b[0;32m     76\u001b[0m                           )\n\u001b[1;32m---> 77\u001b[1;33m             \u001b[0mtarget_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m             \u001b[0mtarget_all\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[0mbatch_states\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[0;32m    129\u001b[0m           method.__name__))\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1567\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1568\u001b[0m       \u001b[1;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1569\u001b[1;33m       data_handler = data_adapter.DataHandler(\n\u001b[0m\u001b[0;32m   1570\u001b[0m           \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1571\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[0;32m   1103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1105\u001b[1;33m     self._adapter = adapter_cls(\n\u001b[0m\u001b[0;32m   1106\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1107\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[0mindices_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"batch\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mslice_inputs\u001b[1;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[0;32m    394\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 396\u001b[1;33m     dataset = dataset.map(\n\u001b[0m\u001b[0;32m    397\u001b[0m         grab_batch, num_parallel_calls=dataset_ops.AUTOTUNE)\n\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[0;32m   1695\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1696\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1697\u001b[1;33m       return ParallelMapDataset(\n\u001b[0m\u001b[0;32m   1698\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1699\u001b[0m           \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[0;32m   4078\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4079\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_use_inter_op_parallelism\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4080\u001b[1;33m     self._map_func = StructuredFunctionWrapper(\n\u001b[0m\u001b[0;32m   4081\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4082\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m   3369\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3370\u001b[0m         \u001b[1;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3371\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3372\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3373\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2936\u001b[0m       \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minputs\u001b[0m \u001b[0mto\u001b[0m \u001b[0mspecialize\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2937\u001b[0m     \"\"\"\n\u001b[1;32m-> 2938\u001b[1;33m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0m\u001b[0;32m   2939\u001b[0m         *args, **kwargs)\n\u001b[0;32m   2940\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2904\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2905\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2906\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2907\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2908\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3213\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3215\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3063\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3064\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3065\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3066\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3067\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    989\u001b[0m       \u001b[1;31m# TensorArrays and `None`s.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 990\u001b[1;33m       func_outputs = nest.map_structure(convert, func_outputs,\n\u001b[0m\u001b[0;32m    991\u001b[0m                                         expand_composites=True)\n\u001b[0;32m    992\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 635\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 635\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    948\u001b[0m               (str(python_func), type(x)))\n\u001b[0;32m    949\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0madd_control_dependencies\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeps_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmark_as_return\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\auto_control_deps.py\u001b[0m in \u001b[0;36mmark_as_return\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[1;31m# of a new identity operation that the stateful operations definitely don't\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;31m# depend on.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m     \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_returned_tensors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[1;34m(input, name)\u001b[0m\n\u001b[0;32m    285\u001b[0m     \u001b[1;31m# variables. Variables have correct handle data when graph building.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m     \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m   \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m   \u001b[1;31m# Propagate handle data for happier shape inference for resource variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_handle_data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[1;34m(input, name)\u001b[0m\n\u001b[0;32m   3999\u001b[0m       \u001b[1;32mpass\u001b[0m  \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4000\u001b[0m   \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4001\u001b[1;33m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[0m\u001b[0;32m   4002\u001b[0m         \"Identity\", input=input, name=name)\n\u001b[0;32m   4003\u001b[0m   \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    740\u001b[0m       \u001b[1;31m# Add Op to graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    741\u001b[0m       \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 742\u001b[1;33m       op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n\u001b[0m\u001b[0;32m    743\u001b[0m                                  \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    744\u001b[0m                                  attrs=attr_protos, op_def=op_def)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[0minp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m       \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    592\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m         compute_device)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m   3475\u001b[0m     \u001b[1;31m# Session.run call cannot occur between creating and mutating the op.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3476\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mutation_lock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3477\u001b[1;33m       ret = Operation(\n\u001b[0m\u001b[0;32m   3478\u001b[0m           \u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3479\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   1972\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mop_def\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1973\u001b[0m         \u001b[0mop_def\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_op_def\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1974\u001b[1;33m       self._c_op = _create_c_op(self._graph, node_def, inputs,\n\u001b[0m\u001b[0;32m   1975\u001b[0m                                 control_input_ops, op_def)\n\u001b[0;32m   1976\u001b[0m       \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[0;32m   1810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1811\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1812\u001b[1;33m     \u001b[0mc_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1813\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1814\u001b[0m     \u001b[1;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from collections import deque\n",
    "\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "Transition = namedtuple(\n",
    "    'Transition', ('state', 'action', 'reward',\n",
    "                   'next_state', 'done'))\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(\n",
    "            self, env, discount_factor=0.95,\n",
    "            epsilon_greedy=1.0, epsilon_min=0.01,\n",
    "            epsilon_decay=0.995, learning_rate=1e-3,\n",
    "            max_memory_size=2000):\n",
    "        self.enf = env\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.n\n",
    "\n",
    "        self.memory = deque(maxlen=max_memory_size)\n",
    "\n",
    "        self.gamma = discount_factor\n",
    "        self.epsilon = epsilon_greedy\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.lr = learning_rate\n",
    "        self._build_nn_model()\n",
    "\n",
    "    def _build_nn_model(self, n_layers=3):\n",
    "        self.model = tf.keras.Sequential()\n",
    "\n",
    "        # Hidden layers\n",
    "        for n in range(n_layers - 1):\n",
    "            self.model.add(tf.keras.layers.Dense(\n",
    "                units=32, activation='relu'))\n",
    "            self.model.add(tf.keras.layers.Dense(\n",
    "                units=32, activation='relu'))\n",
    "\n",
    "        # Last layer\n",
    "        self.model.add(tf.keras.layers.Dense(\n",
    "            units=self.action_size))\n",
    "\n",
    "        # Build & compile model\n",
    "        self.model.build(input_shape=(None, self.state_size))\n",
    "        self.model.compile(\n",
    "            loss='mse',\n",
    "            optimizer=tf.keras.optimizers.Adam(lr=self.lr))\n",
    "\n",
    "    def remember(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        q_values = self.model.predict(state)[0]\n",
    "        return np.argmax(q_values)  # returns action\n",
    "\n",
    "    def _learn(self, batch_samples):\n",
    "        batch_states, batch_targets = [], []\n",
    "        for transition in batch_samples:\n",
    "            s, a, r, next_s, done = transition\n",
    "            if done:\n",
    "                target = r\n",
    "            else:\n",
    "                target = (r +\n",
    "                          self.gamma * np.amax(\n",
    "                            self.model.predict(next_s)[0]\n",
    "                            )\n",
    "                          )\n",
    "            target_all = self.model.predict(s)[0]\n",
    "            target_all[a] = target\n",
    "            batch_states.append(s.flatten())\n",
    "            batch_targets.append(target_all)\n",
    "            self._adjust_epsilon()\n",
    "        return self.model.fit(x=np.array(batch_states),\n",
    "                              y=np.array(batch_targets),\n",
    "                              epochs=1,\n",
    "                              verbose=0)\n",
    "\n",
    "    def _adjust_epsilon(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        samples = random.sample(self.memory, batch_size)\n",
    "        history = self._learn(samples)\n",
    "        return history.history['loss'][0]\n",
    "\n",
    "\n",
    "def plot_learning_history(history):\n",
    "    fig = plt.figure(1, figsize=(14, 5))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    episodes = np.arange(len(history[0])) + 1\n",
    "    plt.plot(episodes, history[0], lw=4,\n",
    "             marker='o', markersize=10)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "    plt.xlabel('Episodes', size=20)\n",
    "    plt.ylabel('# Total Rewards', size=20)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# General settings\n",
    "EPISODES = 200\n",
    "batch_size = 32\n",
    "init_replay_memory_size = 500\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env = gym.make('CartPole-v1')\n",
    "    agent = DQNAgent(env)\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, agent.state_size])\n",
    "\n",
    "    # Filling up the replay-memory\n",
    "    for i in range(init_replay_memory_size):\n",
    "        action = agent.choose_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, agent.state_size])\n",
    "        agent.remember(Transition(state, action, reward,\n",
    "                                  next_state, done))\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            state = np.reshape(state, [1, agent.state_size])\n",
    "        else:\n",
    "            state = next_state\n",
    "\n",
    "    total_rewards, losses = [], []\n",
    "    for e in range(EPISODES):\n",
    "        state = env.reset()\n",
    "        if e % 10 == 0:\n",
    "            env.render()\n",
    "        state = np.reshape(state, [1, agent.state_size])\n",
    "        for i in range(500):\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = np.reshape(next_state,\n",
    "                                    [1, agent.state_size])\n",
    "            agent.remember(Transition(state, action, reward,\n",
    "                                      next_state, done))\n",
    "            state = next_state\n",
    "            if e % 10 == 0:\n",
    "                env.render()\n",
    "            if done:\n",
    "                total_rewards.append(i)\n",
    "                print('Episode: %d/%d, Total reward: %d'\n",
    "                      % (e, EPISODES, i))\n",
    "                break\n",
    "            loss = agent.replay(batch_size)\n",
    "            losses.append(loss)\n",
    "    plot_learning_history(total_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
